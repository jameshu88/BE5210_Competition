{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from scipy.io import loadmat, savemat\n","from scipy.signal import welch\n","from scipy.stats import pearsonr\n","from sklearn.preprocessing import StandardScaler\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# =============================\n","# Dataset Definition\n","# =============================\n","class EcogDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = torch.tensor(X, dtype=torch.float32)\n","        self.Y = torch.tensor(Y, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.Y[idx]\n","\n","# =============================\n","# Attention Layer\n","# =============================\n","class Attention(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.attn = nn.Linear(input_dim, 1)\n","\n","    def forward(self, x):\n","        weights = torch.softmax(self.attn(x), dim=1)\n","        return (x * weights).sum(dim=1)\n","\n","# =============================\n","# Model Definition\n","# =============================\n","class CNNBiLSTMAttn(nn.Module):\n","    def __init__(self, in_ch, seq_len, out_dim):\n","        super().__init__()\n","        self.cnn = nn.Sequential(\n","            nn.Conv1d(in_ch, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(64),\n","            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(128),\n","            nn.Dropout(0.3)\n","        )\n","        self.bilstm = nn.LSTM(128, 64, num_layers=2,\n","                             batch_first=True, bidirectional=True)\n","        self.attn  = Attention(128)\n","        self.fc    = nn.Sequential(\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, out_dim)\n","        )\n","\n","    def forward(self, x):\n","        # x: [B, T, C] â†’ CNN wants [B, C, T]\n","        x = x.permute(0, 2, 1)\n","        x = self.cnn(x)\n","        x = x.permute(0, 2, 1)\n","        x, _ = self.bilstm(x)\n","        x = self.attn(x)\n","        return self.fc(x)\n","\n","# =============================\n","# Feature Extraction\n","# =============================\n","def get_bandpower_feats(ecog, fs, win_len, step_len, bands):\n","    win_s = int(win_len/1000*fs)\n","    step_s = int(step_len/1000*fs)\n","    feats = []\n","    for i in range(0, ecog.shape[0]-win_s+1, step_s):\n","        w = ecog[i:i+win_s]\n","        row = []\n","        for ch in range(w.shape[1]):\n","            f, Pxx = welch(w[:,ch], fs=fs, nperseg=win_s)\n","            for low,high in bands:\n","                idx = (f>=low)&(f<=high)\n","                row.append(np.mean(Pxx[idx]))\n","        feats.append(row)\n","    return np.array(feats)\n","\n","def make_windows(ecog, glove, win_len=1000, step_len=50, fs=1000):\n","    win = int(win_len/1000*fs)\n","    step= int(step_len/1000*fs)\n","    X, Y = [], []\n","    for i in range(0, ecog.shape[0]-win+1, step):\n","        X.append(ecog[i:i+win])\n","        Y.append(np.mean(glove[i:i+win],axis=0))\n","    return np.array(X), np.array(Y)\n","\n","# =============================\n","# Training Function\n","# =============================\n","def train_subject_model(ecog, glove, bands, device):\n","    # 1) Create windows & labels\n","    X_time, Y = make_windows(ecog, glove, win_len=700, step_len=50, fs=1000)\n","    X_freq    = get_bandpower_feats(ecog, 1000, 700, 50, bands)\n","    X_delta   = np.diff(X_time, axis=1, prepend=X_time[:,:1,:])\n","    X_time_full = np.concatenate([X_time, X_delta], axis=2)  # [windows, T, 2*C]\n","\n","    # === REMOVE TIME-DOMAIN STANDARD SCALER! ===\n","    # We no longer flatten & scale X_time_full to save ~8 GB of RAM.\n","    # Rely on BatchNorm inside the CNN to normalize.\n","\n","    # 2) Scale frequency features only\n","    sc_freq  = StandardScaler()\n","    X_freq_s = sc_freq.fit_transform(X_freq)\n","    # Expand to match time dimension\n","    X_freq_e = np.repeat(X_freq_s[:,None,:], X_time_full.shape[1], axis=1)\n","\n","    # 3) Final concatenation\n","    X_final = np.concatenate([X_time_full, X_freq_e], axis=2)  # [windows, T, 2C+F]\n","\n","    # 4) Train/test split\n","    split = int(0.7 * len(X_final))\n","    X_tr, X_te = X_final[:split], X_final[split:]\n","    Y_tr, Y_te = Y[:split], Y[split:]\n","\n","    # 5) DataLoaders with no workers & small batch\n","    bs = 16\n","    train_dl = DataLoader(EcogDataset(X_tr, Y_tr),\n","                          batch_size=bs, shuffle=True,\n","                          num_workers=0, pin_memory=False)\n","    test_dl  = DataLoader(EcogDataset(X_te, Y_te),\n","                          batch_size=bs, shuffle=False,\n","                          num_workers=0, pin_memory=False)\n","\n","    # 6) Build model, optimizer, loss\n","    model = CNNBiLSTMAttn(X_tr.shape[2], X_tr.shape[1], Y_tr.shape[1]).to(device)\n","    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n","    crit  = nn.MSELoss()\n","\n","    # 7) Train loop\n","    for epoch in range(30):\n","        model.train()\n","        for xb, yb in train_dl:\n","            xb, yb = xb.to(device), yb.to(device)\n","            pred   = model(xb)\n","            loss   = crit(pred, yb)\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","    # 8) Evaluate\n","    model.eval()\n","    preds, trues = [], []\n","    with torch.no_grad():\n","        for xb, yb in test_dl:\n","            xb = xb.to(device)\n","            out = model(xb).cpu().numpy()\n","            preds.append(out)\n","            trues.append(yb.numpy())\n","\n","    Y_pred = np.vstack(preds)\n","    Y_true = np.vstack(trues)\n","    r_vals = [pearsonr(Y_true[:,i], Y_pred[:,i])[0] for i in range(Y_true.shape[1])]\n","    mean_r = np.mean([r_vals[i] for i in [0,1,2,4]])\n","    return model, sc_freq, r_vals, mean_r\n","\n","# =============================\n","# Main Execution\n","# =============================\n","if __name__ == \"__main__\":\n","    import warnings\n","    warnings.filterwarnings(\"ignore\")\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(\"Device:\", device)\n","\n","    data = loadmat('/kaggle/input/bcijamesdataset/raw_training_data.mat')\n","    ecog_all = data['train_ecog']\n","    glove_all = data['train_dg']\n","    bands = [(5,15),(20,30),(70,115)]\n","\n","    all_mean = []\n","    for s in range(3):\n","        print(f\"\\n--- Subject {s+1} ---\")\n","        _, _, r_vals, mr = train_subject_model(ecog_all[s,0],\n","                                               glove_all[s,0],\n","                                               bands, device)\n","        print(\"r:\", np.round(r_vals,3), \"mean r (no ring):\", mr)\n","        all_mean.append(mr)\n","\n","    print(\"\\nFinal mean r across subjects:\", np.mean(all_mean))\n","\n","    # Save summary\n","    savemat('model_summary.mat', {'mean_r': np.mean(all_mean)})\n"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
